{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LdaModel(num_terms=24948, num_topics=10, decay=0.5, chunksize=2000)\n",
      "Top 15 terms for topic #0: phone, work, adapter, plug, headphone, jack, 25mm, iphone, music, stereo, cell, product, nokia, listen, 35mm\n",
      "Top 15 terms for topic #1: noise, wind, xbox, truck, shut, highway, speed, traffic, background, 360, youre, loses, standing, live, razr\n",
      "Top 15 terms for topic #2: one, product, work, it, bought, headset, would, great, item, amazon, buy, got, new, first, month\n",
      "Top 15 terms for topic #3: software, dragon, nokia, ipaq, hp, la, 3650, voip, htc, 6600, audio, potential, music, support, amplifier\n",
      "Top 15 terms for topic #4: headset, phone, it, call, use, bluetooth, time, battery, work, button, one, get, device, unit, good\n",
      "Top 15 terms for topic #5: ear, fit, sound, it, piece, like, one, headset, earpiece, good, quality, get, well, comfortable, stay\n",
      "Top 15 terms for topic #6: headset, hear, sound, quality, good, great, it, phone, volume, work, use, bluetooth, noise, well, one\n",
      "Top 15 terms for topic #7: logitech, h500, iogear, hs805, samsung, h, mobile, dy, wep200, dying, apple, syncing, ton, hs810, win\n",
      "Top 15 terms for topic #8: tooth, blue, 25, mm, motorolas, plug, mx150, verizon, sleek, boom, amplifier, planet, oem, flexible, motorcycle\n",
      "Top 15 terms for topic #9: bt250v, e, de, perform, quietspot, cable, que, eliminate, repair, mph, personal, centro, q, threw, bluetrek\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "asins = dict()\n",
    "categories = dict()\n",
    "reviewtextlist = list()\n",
    "\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'r')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "\n",
    "\n",
    "## Read categories,asins for a specific category and keep in\n",
    "## a dictionary\n",
    "def readMetaData(path,catname):\n",
    "    i = 0\n",
    "    for d in parse(path):\n",
    "        numCategories=0\n",
    "    #print d['categories'], len(d['categories'][0])\n",
    "    #print d['title'],d['price'],d['salesrank']\n",
    "        title= \"Not available\"\n",
    "        if 'title' in d:\n",
    "            title=d['title']\n",
    "\n",
    "        if(len(d['categories']) > 0):\n",
    "            numCategories = len(d['categories'][0])\n",
    "            for i in  range(0,numCategories):\n",
    "                cat = d['categories'][0][i]\n",
    "                if cat not in categories:\n",
    "                    categories[cat]=cat\n",
    "                if cat == catname:\n",
    "                    asins[d['asin']] = title\n",
    "        i += 1\n",
    "\n",
    "def readReviewdata(path):\n",
    "    with gzip.open(path, \"rb\") as f:\n",
    "        for line in f:\n",
    "            # print line\n",
    "            d = eval(line)\n",
    "            reviewerId = d['reviewerID']\n",
    "            asin = d['asin']\n",
    "            rating = d['overall']\n",
    "            summary = d['summary']\n",
    "            reviewText = d['reviewText']\n",
    "            dtime = d['reviewTime']\n",
    "            # dt = datetime.datetime.strptime(dtime, \"%m %d, %Y\")\n",
    "            # if len(d['reviewText']) == 0:\n",
    "            #     continue\n",
    "            # if dt.year > year:\n",
    "            if asin in asins and len (reviewText)>0:\n",
    "                reviewtextlist.append(reviewText)\n",
    "\n",
    "\n",
    "def clean(doc):\n",
    "    lemma = WordNetLemmatizer()\n",
    "    #stopset = set(stopwords.words('english'))\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stopwords.words('english')])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in string.punctuation)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    #return stop_free\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def main():\n",
    "    readMetaData('meta_Cell_Phones_and_Accessories.json.gz',\"Headsets\")\n",
    "    #for k, v in asins.iteritems():\n",
    "    #    print k,v\n",
    "    readReviewdata('reviews_Cell_Phones_and_Accessories.json.gz')\n",
    "    reviewId = 0\n",
    "    reviewtextlist_capped = reviewtextlist[0:10000]\n",
    "    for ll in reviewtextlist_capped:\n",
    "        #print reviewId,\":\",ll\n",
    "        reviewId +=1\n",
    "\n",
    "    doc_clean = [clean(reviewText).split() for reviewText in reviewtextlist_capped]\n",
    "    #print doc_clean\n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index.\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    #print(dictionary.token2id)\n",
    "    # Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "    #doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    #print corpus\n",
    "    tfidf = TfidfModel(corpus)\n",
    "    #print tfidf[corpus[0]]\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "\n",
    "    n_topics = 10\n",
    "    #lda = LdaModel(corpus_tfidf, id2word=dictionary, num_topics=n_topics)\n",
    "    lda = LdaModel(corpus, id2word=dictionary, num_topics=n_topics, passes=100)\n",
    "    print lda\n",
    "\n",
    "    for i in range(0, n_topics):\n",
    "        temp = lda.show_topic(i, 15)\n",
    "        terms = []\n",
    "        for term in temp:\n",
    "            terms.append(term)\n",
    "        print \"Top 15 terms for topic #\" + str(i) + \": \"+ \", \".join([i[0] for i in terms])\n",
    "\n",
    "    # document-topic distribution\n",
    "    for doc in corpus:\n",
    "        temp = lda.get_document_topics(doc)\n",
    "    # returns a list of (topic_id, topic_probability) 2-tuples.\n",
    "\n",
    "    # topic-word distribution\n",
    "    for i in range(0, n_topics):\n",
    "        temp = lda.get_topic_terms(i, 15)\n",
    "    # returns a list of (word_id, probability) 2-tuples for the topn most probable words in topic topicid.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
